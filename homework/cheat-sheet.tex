\documentclass[10pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{listings}
\lstset{
  numbers=left,
  numbersep=5pt,
  stepnumber=1,
  tabsize=2,
  showstringspaces=false
}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}
\usepackage[margin=1.5cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\newenvironment{answer}[1]{
  \subsubsection*{Question #1}
}

\begin{document}
\large CS 590 Reference Sheet \hfill Matt Dickenson
\headsep 10pt

\paragraph{Asymptotic Notation}
\begin{itemize}
\item $f(n)=o(g(n)):~ 0 \leq f(n) < cg(n))$
\item $f(n)=O(g(n)):~0 \leq f(n) \leq cg(n)$ (upper bound) 
\item $f(n)=\Theta(g(n)) \leftrightarrow \Omega(g(n))~\&~O(g(n)) $
\item $f(n)=\Omega(g(n):~0 \leq cg(n) \leq f(n)$ (lower bound) 
\item $f(n)=\omega(g(n):~0 \leq cg(n) < f(n)$ 
\end{itemize}

\paragraph{Useful Equations}
\begin{eqnarray*}
\sum_{k=1}^n k &=& \frac{1}{2}n(n+1) = \Theta(n^2) \\
\sum_{k=0}^n p^k &=& {1-p^{k+1} \over 1 - p} \\ 
\text{Stirling:}~~~n! &\approx& \sqrt{2\pi n} ({ n \over e})^n
\end{eqnarray*}

\paragraph{Amortized Analysis}
\begin{eqnarray*}
\hat{c}_i &=& c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
\sum_{i=1}^n \hat{c}_i &=& \sum_{i=1}^n c_i + \Phi(D_n)-\Phi(D_0)
\end{eqnarray*}

\paragraph{Master Method}
For recurrences of the form $T(n)=a T(n/b) + f(n)$, where $a$ is the number of branches at each level, $b$ is the reduction in size at each level, and $f(n)$ is the initial cost:

\newcounter{hcounter}
\begin{list}{\textbf{Case} \arabic{hcounter}:~}{\usecounter{hcounter}}
\item $f(n) = O(n^{\log_b a - \epsilon}) \rightarrow T(n)=\Theta(n \log_b a), \epsilon>0$: $f(n)$ is polynomially slower than $n^{\log_b a}$, weight increases toward leaves because $b<a$
\item $f(n)=\Theta(n^{\log_b a}) \rightarrow T(n)=\Theta(n^{\log_b a} \log^{k+1} n), k\geq 0$: $f(n)~\&~n^{\log_b a}$ grow similarly, weight remains constant because $b=a$
\item $f(n)=\Omega(n^{\log_b a + \epsilon}) \rightarrow T(n)=\Theta(f(n))$: weight decreases because $b>a$
\end{list}

% \pagebreak
% \paragraph{Recursion Trees}

\paragraph{Amortized Analysis} The amortized cost (upper bound) $\hat{c}_i$ for the potential function $\Phi$ and data $D_i$ is defined as:
\begin{eqnarray*}
\hat{c}_i &=& c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
\sum_{i=1}^n \hat{c}_i &=& \sum_{i=1}^n c_i + \Phi(D_n) - \Phi(D_0)
\end{eqnarray*}

\paragraph{Hashing} A hash function is \emph{perfect} if it causes no collisions, but there is no perfect hash function if $|S|>m$. 

Chaining: put a linked list at each of the slots in the table, cost is proportional to length of the lists. Good hash functions:
\begin{eqnarray*}
h(k) = k mod~m \\
h(k) = (ak + b) mod~m \\
h(k) = ((ax + b) mod~p) mod~m
\end{eqnarray*}

A family $H$ of hash functions from $U$ to $M$ is 2-universal if for all $x \neq y$, $Pr(h(x)=h(y)) \leq {1 \over m}$. BUT there are $u^m$ functions from $U$ to $M$, requiring $m \log u$ bits to choose/represent/store. We just pick one at random. For $r$ operations, the expected total work is $r(1+{s \over m})$. 

Good hashing: Let $m$ be a prime number, $(x_0, ..., x_r)$ represent a key $x$, $\bar{a}=(a_0, ..., a_r)$. 

\begin{eqnarray*}
h_{\bar{a}}(x)= (\sum_{i=0}^r a_i x_i) mod~m \\
H = \{ h_{\bar{a}}(x) | a_i \in \{0, ..., m-1 \} \}
\end{eqnarray*}

Open-addressing: Load factor for number of keys $n$ is $\alpha={n \over m}$. Search takes $\Theta(1+\alpha)$ expected time. Because elements are stored in the table itself, the load factor cannot exceed 1. The expected number of probes for inserting in a table (or an unsuccessful search) is $1 \over 1-\alpha$. The expected number of probes for a successful search is ${1 \over \alpha} ln {1 \over 1-\alpha} + {1 \over \alpha}$.

\paragraph{Complexity} $\Pi^\prime$ is poly-time reducivle to $\Pi$ ($\Pi^\prime \leq \Pi$) iff there is a poly-time function $f()$ that maps inputs $x^\prime$ of $\Pi^\prime$ into inputs $x$ of $\Pi$ such that $\Pi^\prime(x^\prime)=\Pi(f(x))$. 
\begin{enumerate}
 \item If $\Pi \in P$ and $\Pi^\prime \leq \Pi$ then $\Pi^\prime \in P$
 \item If $\Pi \in NP$ and $\Pi^\prime \leq \Pi$ then $\Pi^\prime \in NP$
 \item If $\Pi^\prime \leq \Pi$ and $\Pi^{\prime \prime} \leq \Pi^\prime$, then $\Pi^{\prime \prime} \leq \Pi$
\end{enumerate} 

\paragraph{Graph Algorithms} 

A preordering is a list of the vertices in the order that they were first visited by the depth-first search algorithm. 

\begin{table}[h!]
\caption{Uses of Graph Search}
\begin{center}
\begin{tabular}{ll}
DFS & connected components, topological sort \\
BFS & shortest path
\end{tabular}
\end{center}
\end{table}

% \begin{lstlisting}
% DFS(G):
%   for each vertex u in V:
%     u.color = white; u.predecessor = nil
%   t = 0
%   for each vertex u in V:
%     if u.color == white:
%       Visit(G, u)

% Visit(G, u):
%   t = t+1
%   u.d = time
%   u.color = gray
%   for each v in u.neighbors:
%     if v.color == white:
%       v.predecessor = u
%       Visit(G, v)
%   u.color = black
%   t = t+1
%   u.f = t 
% \end{lstlisting}

% \begin{lstlisting}
% BFS(G, s):
%   for each vertex u in V-s:
%     u.color = white; u.d = inf; u.predecessor = nil
%   s.color = gray; s.d = 0; s.pi = nil
%   Q = {s}
%   until Q is empty:
%     u = Q.pop 
%     for each v in u.neighbors:
%       if v.color == white:
%         v.color = gray; v.d = u.d+1; v.predecessor = u 
%         Q.push(v)
%     u.color = black 
% \end{lstlisting}


% Dijkstra:
% \begin{enumerate}
% \item Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes.
% \item Mark all nodes unvisited. Set the initial node as current. Create a set of the unvisited nodes called the unvisited set consisting of all the nodes.
% \item For the current node, consider all of its unvisited neighbors and calculate their tentative distances. 
% \item When we are done considering all of the neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.
% \item If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.
% \item Select the unvisited node that is marked with the smallest tentative distance, and set it as the new ``current node'' then go back to step 3.
% \end{enumerate}


% algs from hw:
% minimum spanning tree
% max flow/min cut


\end{document}
